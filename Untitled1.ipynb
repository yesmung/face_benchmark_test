{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.python.ops import image_ops\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detect_face(file_path):\n",
    "\n",
    "    INPUT_IMAGE_SIZE = 512    \n",
    "    tflite_graph = tf.Graph()\n",
    "    \n",
    "    with tflite_graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            # file_path = \"/home/task1/Desktop/myungsung.kwak/project/DataShare/test_image/face006/face017.jpg\"\n",
    "            fp_placeholder = tf.placeholder(tf.string, name='inputFile')\n",
    "            file_content = tf.read_file(file_path, name='loadFile')\n",
    "            image_data = tf.image.decode_jpeg(file_content, name='decodeJpeg', channels=3)\n",
    "            \n",
    "\n",
    "            resize_bilinear = tf.image.resize_images(image_data, \n",
    "                                                     size=[INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE], \n",
    "                                                     method=tf.image.ResizeMethod.BILINEAR)\n",
    "            \n",
    "            resize_bilinear = (resize_bilinear - 128.0) / 128.0\n",
    "\n",
    "            \n",
    "            actual_resize_bilinear = resize_bilinear.eval(feed_dict={fp_placeholder:file_path})\n",
    "            actual_resize_bilinear = tf.expand_dims(actual_resize_bilinear, 0) \n",
    "            \n",
    "            print(\"actual_resize_bilinear : \", actual_resize_bilinear)\n",
    "            \n",
    "            #input_data = sess.run(actual_resize_bilinear)\n",
    "            #print(\"input_data type :\", type(input_data))\n",
    "            #print(\"input_data shape :\", input_data.shape)\n",
    "\n",
    "            input_data = tf.summary.image(\"input_data\", actual_resize_bilinear, max_outputs=3)\n",
    "            print(\"---------------------\")\n",
    "            print(type(actual_resize_bilinear))\n",
    "            print(actual_resize_bilinear.shape)\n",
    "            print(\"---------------------\")\n",
    "\n",
    "            # tflite_model_path = \"/home/task1/Desktop/myungsung.kwak/target/ssdlite_mobilenet_v2_coco_2018_05_09/output/detect.tflite\"\n",
    "            tflite_model_path = \"/home/task1/Desktop/myungsung.kwak/testgit/ssd_mobilenetv1/20190403/tflite/detect.tflite\"\n",
    "\n",
    "            # interpreter = tf.lite.Interpreter(model_path=\"models/ssdlite/trained_ssdlite_mobilenet_v2_414114.tflite\")\n",
    "            interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "            interpreter.allocate_tensors()\n",
    "\n",
    "            input_details = interpreter.get_input_details() # input image\n",
    "            output_details = interpreter.get_output_details() # box, score, class, num detections\n",
    "            tensor_details = interpreter.get_tensor_details()\n",
    "\n",
    "    #         print(\"========== Input details ==========\")\n",
    "    #         for name in input_details:\n",
    "    #             print(name)\n",
    "\n",
    "    #         print(\"========== Output details ==========\")\n",
    "    #         for name in output_details:\n",
    "    #             print(name)\n",
    "\n",
    "    #         print(\"========== Tensor details ===========\")\n",
    "    #         for name in tensor_details:\n",
    "    #             print(name)     \n",
    "\n",
    "            input_data = sess.run(actual_resize_bilinear)\n",
    "            interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "            interpreter.invoke()\n",
    "\n",
    "            boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "            classes = interpreter.get_tensor(output_details[1]['index']) # is it right?\n",
    "            scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "            num_detections = interpreter.get_tensor(output_details[3]['index'])\n",
    "            \n",
    "            print(type(boxes))\n",
    "            print(type(classes))\n",
    "            print(type(scores))\n",
    "            print(type(num_detections))\n",
    "            \n",
    "\n",
    "            boxes = np.squeeze(boxes)\n",
    "            scores = np.squeeze(scores)\n",
    "\n",
    "            filtered_score_index = np.argwhere(scores >= 0.85).flatten()\n",
    "\n",
    "            print(\"- filtered_score_index : \",filtered_score_index)\n",
    "            selected_boxes = boxes[filtered_score_index]\n",
    "            selected_scores = scores[filtered_score_index]\n",
    "\n",
    "            faces = np.array([[\n",
    "                int(x1 * w),\n",
    "                int(y1 * h),\n",
    "                int(x2 * w),\n",
    "                int(y2 * h),\n",
    "            ] for y1, x1, y2, x2 in selected_boxes])\n",
    "\n",
    "            print(\"faces : \", faces)\n",
    "            print(\"selected_boxes : \", selected_boxes)\n",
    "            print(\"selected_scores : \", selected_scores)\n",
    "            \n",
    "            return faces, selected_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_resize_bilinear :  Tensor(\"ExpandDims:0\", shape=(1, 512, 512, 3), dtype=float32)\n",
      "---------------------\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(1, 512, 512, 3)\n",
      "---------------------\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "- filtered_score_index :  [0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'Dimension' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-97d39fb59697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/task1/Desktop/myungsung.kwak/project/DataShare/test_image/face006/face021.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-45bf22f9355c>\u001b[0m in \u001b[0;36mdetect_face\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             ] for y1, x1, y2, x2 in selected_boxes])\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"faces : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-45bf22f9355c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             ] for y1, x1, y2, x2 in selected_boxes])\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"faces : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__rmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    408\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mDimension\u001b[0m \u001b[0mwhose\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mother\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \"\"\"\n\u001b[0;32m--> 410\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__floordiv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'Dimension' and 'float'"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/task1/Desktop/myungsung.kwak/project/DataShare/test_image/face006/face021.jpg\"\n",
    "\n",
    "faces, scores = detect_face(file_path)\n",
    "\n",
    "image_data = cv2.imread(file_path)\n",
    "h, w, c = image_data.shape\n",
    "\n",
    "green = (0, 255, 0)\n",
    "\n",
    "print(\"=============================================================\")\n",
    "for face in faces:\n",
    "    xmin = face[0]\n",
    "    ymin = face[1]\n",
    "    xmax = face[2]\n",
    "    ymax = face[3]\n",
    "    cv2.rectangle(image_data, (xmin, ymin), (xmax, ymax), green, 4)\n",
    "    \n",
    "cv2.imwrite(\"./tflite_test/face21.jpg\", image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
